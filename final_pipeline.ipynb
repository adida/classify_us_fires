{"cells":[{"cell_type":"markdown","metadata":{"id":"b5K8FqgW61Pt"},"source":["# import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oXHv_R5_D2CS"},"outputs":[],"source":["import pandas as pd\n","import numpy as np \n","import matplotlib.pyplot as plt\n","import sklearn\n","%matplotlib inline\n","plt.style.use('ggplot')\n","import warnings\n","warnings.filterwarnings('ignore')\n","import matplotlib\n","import pickle\n","from sklearn.preprocessing import OneHotEncoder\n","from datetime import datetime, date\n","from numpy.core._exceptions import UFuncTypeError\n","from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n","from scipy import sparse\n","from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.metrics import roc_auc_score\n","from xgboost import plot_importance\n","from collections import Counter\n","from scipy.spatial import distance_matrix,distance\n","\n","matplotlib.rcParams['font.size'] = 14\n","matplotlib.rcParams['figure.figsize'] = (18,8)"]},{"cell_type":"markdown","source":["# Functions"],"metadata":{"id":"hu6FoM51vmkA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEi9UEkk4Wdz"},"outputs":[],"source":["def clean_data(df,features_to_delete):\n","  return df.drop(columns=features_to_delete, errors='ignore')"]},{"cell_type":"markdown","metadata":{"id":"ZyvHdyiMGjyx"},"source":["A function to Redoce cardinality of given column\n","\n","return the columns with the categories that captured {threshold}% of the data, and puts 'other' in the rest."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jFi_nxIUGjN9"},"outputs":[],"source":["\n","def cumulatively_categorise(column,threshold=0.75,return_categories_list=True,other_name='Other'):\n","  #Find the threshold value using the percentage and number of instances in the column\n","  threshold_value=int(threshold*len(column))\n","  #Initialise an empty list for our new minimised categories\n","  categories_list=[]\n","  #Initialise a variable to calculate the sum of frequencies\n","  s=0\n","  #Create a counter dictionary of the form unique_value: frequency\n","  counts=Counter(column)\n","\n","  #Loop through the category name and its corresponding frequency after sorting the categories by descending order of frequency\n","  for i,j in counts.most_common():\n","    #Add the frequency to the global sum\n","    s+=dict(counts)[i]\n","    #Append the category name to the list\n","    categories_list.append(i)\n","    #Check if the global sum has reached the threshold value, if so break the loop\n","    if s>=threshold_value:\n","      break\n","  #Append the category Other to the list\n","  categories_list.append(other_name)\n","\n","  #Replace all instances not in our new categories by Other  \n","  new_column=column.apply(lambda x: x if x in categories_list else other_name)\n","\n","  #Return transformed column and unique values if return_categories=True\n","  if(return_categories_list):\n","    return new_column,categories_list\n","  #Return only the transformed column if return_categories=False\n","  else:\n","    return new_column"]},{"cell_type":"markdown","metadata":{"id":"HyT2DEcg1spn"},"source":["## convert to date_time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aWduNasZ1spo"},"outputs":[],"source":["def fix_julian_date(df, col):\n","    \"\"\"\n","    converts dates from Julian format (float) to datetime\n","    col - name of the column to convert\n","    \"\"\"\n","    epoch = pd.to_datetime(0, unit='s').to_julian_date()\n","    disc_dates = df[col]\n","    df[col] = pd.to_datetime(disc_dates - epoch, unit='D')"]},{"cell_type":"code","source":["def adding_dates_data(df, features_to_delete):\n","  try:\n","    fix_julian_date(df, 'DISCOVERY_DATE')\n","    fix_julian_date(df, 'CONT_DATE')\n","  except UFuncTypeError as e:\n","    print('dates allready converted')\n","\n","  df['DISCOVERY_WOY'] = df['DISCOVERY_DATE'].dt.week\n","  df['CONT_WOY'] = df['CONT_DATE'].dt.week\n","  df['DISCOVERY_MONTH'] = df['DISCOVERY_DATE'].dt.month\n","  df['CONT_MONTH'] = df['CONT_DATE'].dt.month\n","  try:\n","    df['CONT_TIME'] = df['CONT_TIME'].apply(lambda x: pd.to_datetime(x, format='%H%M').time() if x else None)\n","    df['DISCOVERY_TIME'] = df['DISCOVERY_TIME'].apply(lambda x: pd.to_datetime(x, format='%H%M').time() if x else None)\n","  except ValueError as e:\n","    print('conversion of CONT_TIME and/or DISCOVERY_TIME - failed')\n","\n","  \n","  return df"],"metadata":{"id":"GXpvxs_Q1spo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Engineering Functions"],"metadata":{"id":"yyjrIfLRgSLD"}},{"cell_type":"code","source":["def features_to_one_hot(df, one_hot_features, ohe, is_test):\n","  \"\"\"\n","  Switches the feature representation to one hot and deletes the original\n","  \"\"\"\n","  if not is_test:\n","    ohe_arr = ohe.fit_transform(df[one_hot_features])\n","  else:\n","    ohe_arr = ohe.transform(df[one_hot_features])\n","  # Getting all features names\n","  feature_labels = ohe.categories_\n","  labels = []\n","  for i, category in enumerate(feature_labels):\n","    for sub in category.ravel():\n","      if sub:\n","        labels.append(one_hot_features[i] + \"_\" + sub)\n","\n","  df_no_onehot = df.drop(columns=one_hot_features)\n","  all_cols = list(df_no_onehot.columns)\n","  all_cols.extend(labels)\n","\n","\n","  df_as_arr = np.array(df_no_onehot,dtype=float)\n","  sparse_df = sparse.csr_matrix(df_as_arr)\n","  final_df = sparse.hstack([sparse_df,ohe_arr])\n","  return final_df, all_cols\n"],"metadata":{"id":"fljJypNWFWtA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def partial_one_hot(df, column_name, encode_values, drop=False):\n","  column_names = []\n","  \n","  for value in encode_values:\n","    column_names.append(f\"{column_name}_{value}\")\n","    df[column_names[-1]] = np.where(df[column_name]==value,1,0)\n","  \n","  if drop:\n","    df.drop(columns={column_name}, inplace=True)\n","  \n","  return column_names"],"metadata":{"id":"wsl7FFFa6qyN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3CGuToO2g90S"},"source":["### Adding features\n","feature engeneering - add features to the data frame"]},{"cell_type":"code","source":["def calc_duration(row):\n","  for value in ['CONT_DATE', 'CONT_TIME', 'DISCOVERY_TIME', 'DISCOVERY_DATE']:\n","    if row[value] is None:\n","      return\n","\n","  return datetime.combine(row['CONT_DATE'], row['CONT_TIME']) - datetime.combine(row['DISCOVERY_DATE'], row['DISCOVERY_TIME'])"],"metadata":{"id":"mwpMnkDpZhAp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def is_holiday(df, date_column):\n","  cal = calendar()\n","  holidays = cal.holidays(start=df[date_column].min(), end=df[date_column].max())\n","  df[f'isHoliday_{date_column}'] = df[date_column].isin(holidays)"],"metadata":{"id":"lxDrrUJp0y3W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cyclical_transform_datetime_feature(df, datetime_feature):\n","    seconds_in_day = 24*60*60\n","    seconds_in_week = 7 * seconds_in_day\n","\n","    df[datetime_feature + '_time_in_seconds'] = pd.to_datetime(df[datetime_feature]).values.astype(np.int64) // 10**6\n","\n","    df[datetime_feature + '_day_sin_time'] = np.sin(2*np.pi*df[datetime_feature + '_time_in_seconds']/seconds_in_day)\n","    df[datetime_feature + '_day_cos_time'] = np.cos(2*np.pi*df[datetime_feature + '_time_in_seconds']/seconds_in_day)\n","    df[datetime_feature + '_sin_time_week'] = np.sin(2*np.pi*df[datetime_feature + '_time_in_seconds']/seconds_in_week)\n","    df[datetime_feature + '_cos_time_week'] = np.cos(2*np.pi*df[datetime_feature + '_time_in_seconds']/seconds_in_week)"],"metadata":{"id":"Epy-kl2rvRIV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cyclical_transform_time_feature(df, hour_feature_name, features_to_delete):\n","  df.hour_feature_name = df_train[hour_feature_name].apply(lambda x: x.hour + x.minute/60 if not x is None else None)\n","\n","  df[f'{hour_feature_name}_hr_sin'] = np.sin(df.hour_feature_name*(2.*np.pi/24))\n","  df[f'{hour_feature_name}_hr_cos'] = np.cos(df.hour_feature_name*(2.*np.pi/24))\n","\n","  features_to_delete.append(hour_feature_name)"],"metadata":{"id":"DjwlinSDFgbJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def increased_poweline(row):\n","  if row['FIRE_YEAR'] > 2009 or (row['FIRE_YEAR'] == 2009 and row['DISCOVERY_WOY'] >=4):\n","    return 1  \n","  return 0\n","\n","def missing_values_decrease(row):\n","  if row['FIRE_YEAR'] > 1996 and row['FIRE_YEAR'] < 2003 or (row['FIRE_YEAR'] == 1996 and row['DISCOVERY_WOY'] >= 41):\n","    return 1\n","\n","  if row['FIRE_YEAR'] > 2012 and row['FIRE_YEAR'] < 2015:\n","    return 1\n","  \n","  if (row['FIRE_YEAR'] == 2012 and row['DISCOVERY_WOY'] <= 9) and (row['FIRE_YEAR'] == 2015 and row['DISCOVERY_WOY'] <= 33):\n","    return 1\n","\n","  return 0\n","\n","def decrease_rail(row):\n","  if row['FIRE_YEAR'] >= 1992 and row['FIRE_YEAR'] < 2002:\n","    return 1\n","  if row['FIRE_YEAR'] == 2002 and row['DISCOVERY_WOY'] < 41:\n","    return 1\n","  return 0"],"metadata":{"id":"xcVjJh4OFk9W"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l5UjkSJdhlNk"},"source":["def add_features(df, good_has_features, features_to_delete):\n","  \"\"\"\n","  Adds features to the dataframe\n","  \"\"\"\n","  # Time\n","  df['WEEKDAY'] =  df['DISCOVERY_DATE'].dt.weekday  # add day of week column\n","\n","  cyclical_transform_datetime_feature(df, 'DISCOVERY_DATE')\n","  cyclical_transform_datetime_feature(df, 'CONT_DATE')\n","  #cyclical_transform_time_feature(df, 'CONT_TIME', features_to_delete)\n","  #cyclical_transform_time_feature(df, 'DISCOVERY_TIME', features_to_delete)\n","\n","  df['Area_of_independent'] = ((df['DISCOVERY_WOY'] > 23) & (df['DISCOVERY_WOY'] < 27)).astype(int)\n","  df['increased_poweline_cases'] = df.apply(increased_poweline, axis=1)\n","  df['decreased_missing_values'] = df.apply(missing_values_decrease, axis=1)\n","  df['decreased_railroad'] = df.apply(decrease_rail, axis=1)\n","  df['increased_lighting_cases'] = df['DISCOVERY_WOY'].apply(lambda x:1 if x >= 19 and x<= 41 else 0)\n","\n","\n","  df['DURATION'] = df.apply(calc_duration,axis=1).astype('timedelta64[s]')\n","\n","  # scale the DURATION\n","  df['DURATION'] = (df['DURATION']-df['DURATION'].min())/(df['DURATION'].max()-df['DURATION'].min())\n","\n","  # Adding holidays\n","  is_holiday(df, \"DISCOVERY_DATE\")\n","  is_holiday(df, \"CONT_DATE\")\n","\n","  # \"has\" features\n","  for feature in good_has_features:\n","    df = add_has_feature(df, feature)\n","  # # remove original features\n","  df = df.drop(good_has_features, axis=1, errors='ignore')\n","\n","\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dindx73xmkVP"},"source":["A function to add has_feature feature"]},{"cell_type":"code","metadata":{"id":"EASvYqTEmn61"},"source":["def add_has_feature(df, feature):\n","  \"\"\"\n","  Adds has_feature to the df while removing the feature itself\n","  \"\"\"\n","  df[f\"HAS_{feature}\"] = df[feature].isna()\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reducing_cardinality(df,feature_to_reduce_cardinality,thershold):\n","  categories_list_dict = {}\n","  for feature in feature_to_reduce_cardinality:\n","    new_col, categories_list = cumulatively_categorise(df[feature], thershold, other_name=f'Other_{feature}')\n","    df[feature] = new_col\n","    categories_list_dict[feature] = categories_list\n","\n","  return df, categories_list_dict"],"metadata":{"id":"Chyy1qcilFB9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reduce_cardinality_test(df,feature_to_reduce_cardinality,categories_list_dict):\n","  for feature in feature_to_reduce_cardinality:\n","    lst = categories_list_dict[feature]\n","    df[feature] = df[feature].apply(lambda x : x if x in lst else f'Other_{feature}')\n","  return df"],"metadata":{"id":"oKD7oB3tF2LA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Final preprocess function"],"metadata":{"id":"ByjmXn0MuO6t"}},{"cell_type":"code","metadata":{"id":"HuVwG7wrf97N"},"source":["def preprocess(df, features_to_delete, good_has_features, one_hot_features,feature_to_reduce_cardinality,ohe,is_test, categories_list_dict=None):\n","  \"\"\"\n","  Returns the df after preprocessing.\n","  \"\"\"\n","  print(\"Starting data processing\\n\\t-----\")\n","  Y = None\n","  if is_test and 'STAT_CAUSE_DESCR' in df.columns:\n","    Y = df['STAT_CAUSE_DESCR']\n","    df.drop(columns=['STAT_CAUSE_DESCR'] ,inplace=True, errors='ignore')\n","  print(\"Starting date processing\")\n","\n","  df = adding_dates_data(df, features_to_delete)\n","\n","  print(\"Finished date processing\")\n","\n","\n","  print(\"Starting adding features\")\n","  df = add_features(df, good_has_features, features_to_delete)\n","  print(\"Finished adding feature\")\n","\n","\n","  print(\"Starting cleaning data\")\n","  df = clean_data(df, features_to_delete)\n","  print(\"Finished cleaning data\")\n","  \n","  #Adding one hot representation for not all values\n","  print(\"Starting reducing cardinality\")\n","  if not is_test:\n","    df, categories_list_dict = reducing_cardinality(df,feature_to_reduce_cardinality,thershold=0.8)\n","  else:\n","    df = reduce_cardinality_test(df,feature_to_reduce_cardinality,categories_list_dict)\n","  print(\"Finished reducing cardinality\")\n","  if not is_test:\n","    Y = df['STAT_CAUSE_DESCR']\n","    df.drop(columns=['STAT_CAUSE_DESCR'], inplace=True, errors='ignore')\n","  # Adding one hot\n","  print(\"Starting adding one hot\")\n","  df_as_sparse , feature_labels = features_to_one_hot(df, one_hot_features, ohe, is_test) \n","  print(\"Finished adding one hot\")\n","  print(\"\\t-----\\nFinished data processing\")\n","  if is_test:\n","    return df_as_sparse,Y\n","  return df_as_sparse, Y, feature_labels, categories_list_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Class "],"metadata":{"id":"7L4ZJ-Se26mI"}},{"cell_type":"code","source":["class FiresPreprocessor:\n","\n","  def __init__(self):\n","  \n","    self.ohe = OneHotEncoder(categories='auto', handle_unknown='ignore', sparse = True)\n","    \n","    self.categories_list_dict = None \n","\n","    self.good_has_features = ['FIRE_NAME','FIRE_CODE']\n","\n","    self.feature_to_reduce_cardinality = ['NWCG_REPORTING_UNIT_NAME','SOURCE_SYSTEM', \"FIPS_NAME\", 'SOURCE_REPORTING_UNIT'] \n","\n","    self.one_hot_features = self.feature_to_reduce_cardinality + ['OWNER_DESCR', 'STATE', 'NWCG_REPORTING_AGENCY']\n","\n","    self.features_to_delete = [\"FOD_ID\", \"FPA_ID\", \"OBJECTID\", \"LOCAL_FIRE_REPORT_ID\", \n","                            \"LOCAL_INCIDENT_ID\", \"ICS_209_INCIDENT_NUMBER\", \"ICS_209_NAME\", \"MTBS_ID\", 'NWCG_REPORTING_UNIT_ID', \n","                            'FIRE_SIZE_CLASS', \"OWNER_CODE\", 'COMPLEX_NAME', 'FIPS_CODE', \n","                            'COUNTY', 'Shape', 'SOURCE_SYSTEM_TYPE', 'CONT_DOY', 'DISCOVERY_DOY', \"MTBS_FIRE_NAME\",'SOURCE_REPORTING_UNIT_NAME', \n","                            'DISCOVERY_DATE', 'CONT_DATE','DISCOVERY_TIME', 'CONT_TIME','STAT_CAUSE_CODE']\n","\n","\n","  def fit_transform(self, data):\n","    X_train, y_train, feature_labels, categories_list_dict = preprocess(data, \n","                                                                        self.features_to_delete, \n","                                                                        self.good_has_features, \n","                                                                        self.one_hot_features,\n","                                                                        self.feature_to_reduce_cardinality, \n","                                                                        self.ohe,is_test=False)\n","    self.categories_list_dict = categories_list_dict\n","    return X_train, y_train\n","\n","  def transform(self,test_data):\n","    X_test, y_test = preprocess(test_data, \n","                                self.features_to_delete, \n","                                self.good_has_features, \n","                                self.one_hot_features,\n","                                self.feature_to_reduce_cardinality, \n","                                self.ohe,True, self.categories_list_dict)\n","    return X_test, y_test\n"],"metadata":{"id":"oJOsusVr5hfX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FiresModel:\n","\n","  def __init__(self, model_type):\n","    self.parameters = { 'colsample_bytree': 0.9911191130678206,\n","                       'eta': 0.49685865594047707,\n","                       'gamma': 1.4308614729557847,\n","                       'max_depth': int(6),\n","                       'min_child_weight': int(4),\n","                       'n_estimators': int(196),\n","                       'reg_alpha': int(40.0),\n","                       'reg_lambda': 0.91095884549275\n","                       }\n","    self.Preprocessor = FiresPreprocessor()\n","    self.model_type = model_type\n","    if self.model_type == 'xgb':\n","      self.model = XGBClassifier(**self.parameters)  \n","    elif self.model_type == 'clf':\n","      self.model = RandomForestClassifier() \n","\n","  def fit(self,data):\n","    X_train, y_train = self.Preprocessor.fit_transform(data)\n","\n","    print(\"begin trainning\")\n","    self.model.fit(X_train, y_train)\n","    print(\"finish trainning\")\n","\n","    y_hat = self.model.predict_proba(X_train)\n","    self.train_score = roc_auc_score(y_train, y_hat, average='macro', multi_class='ovr')\n","\n","  def get_train_score(self):\n","    return self.train_score\n","\n","  def predict(self,test_data):\n","    X_test, y_test = self.Preprocessor.transform(test_data)\n","    y_hat = self.model.predict_proba(X_test)\n","    return y_hat\n","\n","  def score(self,data):\n","    X_test, y_test = self.Preprocessor.transform(data)\n","    y_hat = self.model.predict_proba(X_test)\n","    score = roc_auc_score(y_test, y_hat,  average='macro', multi_class='ovr')\n","    return score\n","\n","\n","\n","  \n","\n","\n","\n"],"metadata":{"id":"ej-K0Icy9mOx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Your code goes Here:"],"metadata":{"id":"q9AGOpHnbBqN"}},{"cell_type":"markdown","source":["## Our way to load the data"],"metadata":{"id":"6F8J9Ypybxin"}},{"cell_type":"code","source":["# Mount drive in google colab\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1DKOwJv8bgEA","executionInfo":{"status":"ok","timestamp":1642174253929,"user_tz":-120,"elapsed":2052,"user":{"displayName":"Ohad Tayler","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE4fNXdlREW-RdXLUMViWjtNIIGUS6FDS1vGbI=s64","userId":"07738643181645355525"}},"outputId":"7a25d98b-d755-4ea3-ce5e-7a0e5ff4d1a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["drive_project_dir = '/content/drive/MyDrive/APLDS/'\n","\n","df_train_pickle_fn = 'df_train_raw.pickle'\n","df_train_pickle_path_raw = drive_project_dir + df_train_pickle_fn\n","df_test_pickle_fn = 'df_test_raw.pickle'\n","df_test_pickle_path = drive_project_dir + df_test_pickle_fn\n","\n","with open(df_train_pickle_path_raw, 'rb') as f: \n","     df_train = pickle.load(f)\n","with open(df_test_pickle_path, 'rb') as f:\n","     df_test = pickle.load(f)"],"metadata":{"id":"ZsfXxj5pbhX4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Use example "],"metadata":{"id":"j9L0IMkFb6Ed"}},{"cell_type":"code","source":["# Use example \n","# We assume we have df_train and df_test.\n","# Our Labels is The column 'STAT_CAUSE_DESCR' and we assume it's inside the DataFrames.\n","model = FiresModel(model_type='xgb')\n","model.fit(df_train.sample(150000))\n","print(f'train AUC score: {model.train_score}\\n')\n","test_score = model.score(df_test)\n","print(f'test AUC score: {test_score}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h-A_6gI-bA81","executionInfo":{"status":"ok","timestamp":1642175359147,"user_tz":-120,"elapsed":1088709,"user":{"displayName":"Ohad Tayler","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE4fNXdlREW-RdXLUMViWjtNIIGUS6FDS1vGbI=s64","userId":"07738643181645355525"}},"outputId":"a334c2f0-90dd-4e09-9321-9d6cd50abe6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting data processing\n","\t-----\n","Starting date processing\n","Finished date processing\n","Starting adding features\n","Finished adding feature\n","Starting cleaning data\n","Finished cleaning data\n","Starting reducing cardinality\n","Finished reducing cardinality\n","Starting adding one hot\n","Finished adding one hot\n","\t-----\n","Finished data processing\n","begin trainning\n","finish trainning\n","train AUC score: 0.8829255325197909\n","Starting data processing\n","\t-----\n","Starting date processing\n","Finished date processing\n","Starting adding features\n","Finished adding feature\n","Starting cleaning data\n","Finished cleaning data\n","Starting reducing cardinality\n","Finished reducing cardinality\n","Starting adding one hot\n","Finished adding one hot\n","\t-----\n","Finished data processing\n","test AUC score: 0.8677210401365572\n"]}]}],"metadata":{"colab":{"collapsed_sections":["b5K8FqgW61Pt","hu6FoM51vmkA","HyT2DEcg1spn","3CGuToO2g90S"],"name":"narrowed pipeline.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}